---
title: "phone_churn"
output: html_notebook
---

Make sure there is no error message.
```{r}
library(mlr)
library(ggplot2)
library(robustHD)
library(readr)
library(caret)
library(spFSR)
library(jsonlite)
data <- read_csv("E:/bitbucket_warehouse/side_projects/phone_churn/dataset/ACMETelephoneABT.csv")
```

We call a dataset imbalanced when the target variable is dominated by one class. Always, we more care about the minority class(es) than the dominant (majority) class. When we train a model, the training aims to reduce the error rate.

Here is an extreme example. When data is very imbalanced, say, 99% of the observations are in the majority class, a decision tree will likely to not split at all and predict anything as a majority class case. This model is certainly not useful and we need fix the problem by making the training data more balanced.

Adjusting the classfication threshold in order to create more minority predictions is a good fix, but more can be done. We can make the dataset less imbalanced so that the model really learn about the characteristics of the minority class observations.

This dataset is perfectly balanced as the company that provided this dataset had already undersampled the majority class (customer who stay), while keep all minority customer rows (churned customers). In reality, we may instead oversample the abnormal customers, while keep all normal customers. It is not necessary to make two classes 50% 50% balanced.

For decision tree, we will use the balanced data. For KNN and Naive Bayes, we will restore the original proportion of classes (90% 10%), by undersampling the churn class.

* Data the company has:     Churn 5000, Not churn 45000
* Data the author provided: Churn 5000, Not churn 5000
* Data for Decision Tree:   Churn 5000, Not churn 5000
* Data for NB and KNN:      Churn 555, Not churn 5000


```{r}
table(data$churn)
```

# Data processing
Notice that only columns that need processing are mentioned.

### How many values are missing in each column?
```{r}
sapply(data, function(x) sum(is.na(x)))
```

### occupation
Too many NAs in occupation. Assign missing. No imputing.
```{r}
data$occupation[is.na(data$occupation)] = 'missing'
```

### This is an unnecessary index column.
```{r}
data$customer = NULL
```

### regionType
```{r}
table(data$regionType)
```

```{r}
data$regionType[data$regionType == 'r'] = 'rural'
data$regionType[data$regionType == 's'] = 'suburban'
data$regionType[data$regionType == 't'] = 'town'
data$regionType[data$regionType == 'unknown'] = NA
```


### creditCard
```{r}
table(data$creditCard)
```

```{r}
data$creditCard[data$creditCard == 'f'] = 'false'
data$creditCard[data$creditCard == 'no'] = 'false'
data$creditCard[data$creditCard == 't'] = 'true'
data$creditCard[data$creditCard == 'yes'] = 'true'
```

### numHandsets

numHandsets is just one of numerical columns which require different handling for different models.

1. For Logistic Regression, we need to bin any value >= say 3 as 3 (as a bin). 
2. For Naive Bayes, naiveBayes from e1071 assumes that the numerical variable is normally distributed, but here numHandsets follows Poisson distribution (many other columns follow Zero-inflated Poisson or Exponential distribution). Therefore, we can bin any value >= say 3 as a bin, and then dummify this column. Alternatively, we can apply Box-cox transformation on the data in hope that it will become normal.
3. No need to do anything to this column if we use tree based models.

```{r}
table(data$numHandsets)
```

### currentHandsetPrice

Is zero currentHandsetPrice due to value being missing or customers getting free phones from mobile plans? Choosing the later is safer. We will not set zero as NA.

```{r}
table(data$currentHandsetPrice)
```

### age

```{r}
hist(data$age)
```

```{r}
data$age[data$age == 0] = NA
```


## Imputation

Imputer requires factors for non-numerical columns.
```{r}
data = as.data.frame(unclass(data), stringsAsFactors=TRUE)
```

See a whole list of available imputation learners by running the two lines in below.

* For numerical missing values:   listLearners("regr", properties = "missings")[c("class", "package")]
* For categorical missing values: listLearners("classif", properties = "missings")[c("class", "package")]

This will take 10 mins to run.
```{r}
imp = impute(data, 
             target = "churn", # Tell the algorithm not to use information in the target column for imputation.
             cols = list(
                 regionType = imputeLearner("classif.cforest"),
                 age = imputeLearner("regr.cforest")
             )
            )
```

Let's see how is the result?
The attempt to impute regionType seems to be a failure. Almost all missing values were assigned to "suburban".
```{r}
table(imp$data$regionType)
```

The attempt to impute age seems good.
```{r}
hist(imp$data$age)
```

Now imp$data is our new data.
Also, give up imputing regionType. Simply fill missing values in regionType by any string.
```{r}
data_imputed = imp$data

data_imputed$regionType = data$regionType
levels(data_imputed$regionType) = c(levels(data_imputed$regionType), 'missing')
data_imputed$regionType[is.na(data_imputed$regionType)] = 'missing'
```

### Now if we jump to run the "Dummify" chunk, the data will be ready for tree based models. Here we just need more processing steps to get it ready for Naive Bayes and KNN.

## Box-cox Transformation, a special handling for this example

You will see that it does not solve the problem in this case, but it is a general way to make a column normally distributed.

First, we define a function which takes a column, transforms and returns the transformed column.

```{r}
BoxCoxTransformation = function(original_data) {
  lambda = BoxCoxTrans(original_data)$lambda
  if (lambda == 0) {
    return(log(original_data))
  } else {
    return((original_data ** lambda - 1)/ lambda)
  }
}
```

Before:

```{r}
hist(data_imputed$numHandsets)
```

After:

```{r}
hist(BoxCoxTransformation(data_imputed$numHandsets))
```

```{r}
data_imputed$numHandsets = BoxCoxTransformation(data_imputed$numHandsets)
data_imputed$currentHandsetPrice = BoxCoxTransformation(data_imputed$currentHandsetPrice + 1)
data_imputed$avgOverBundleMins = BoxCoxTransformation(data_imputed$avgOverBundleMins + 1)
data_imputed$avgRoamCalls = BoxCoxTransformation(data_imputed$avgRoamCalls + 1)
data_imputed$avgReceivedMins = BoxCoxTransformation(data_imputed$avgReceivedMins + 1)
data_imputed$avgOutCalls = BoxCoxTransformation(data_imputed$avgOutCalls + 1)
data_imputed$avgInCalls = BoxCoxTransformation(data_imputed$avgInCalls + 1)
data_imputed$peakOffPeakRatio = BoxCoxTransformation(data_imputed$peakOffPeakRatio + 1)
data_imputed$avgDroppedCalls = BoxCoxTransformation(data_imputed$avgDroppedCalls + 1)
data_imputed$lastMonthCustomerCareCalls = BoxCoxTransformation(data_imputed$lastMonthCustomerCareCalls + 1)
data_imputed$numRetentionCalls = BoxCoxTransformation(data_imputed$numRetentionCalls + 1)
data_imputed$numRetentionOffersAccepted = BoxCoxTransformation(data_imputed$numRetentionOffersAccepted + 1)
data_imputed$newFrequentNumbers = BoxCoxTransformation(data_imputed$newFrequentNumbers + 1)
```

## Standardisation

```{r}
numerical_col_index <- unlist(lapply(data_imputed, is.numeric))  # For all numerical columns
data_standardised = lapply(data_imputed[ , numerical_col_index], standardize) # Standardise it
```

Check if mean is close to 0 and standard deviation is 1.
```{r}
mean(data_standardised$currentHandsetPrice)
sd(data_standardised$currentHandsetPrice)
```

Combine the standardised numerical column dataframe with the rest, i.e. the categorical column dataframe.
```{r}
categorical_col_index = !numerical_col_index
data_standardised = cbind(data_standardised, data_imputed[ , categorical_col_index])
```


## Dummify

In this example, for each categorical variable column, for n factor levels there will be n - 1 dummy variables (the first factor level will be dropped).
```{r}
data_dummified = createDummyFeatures(data_standardised, target = 'churn', method = "reference", cols = NULL)
```




